{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1AHtR5YtOwTXMbLQprUTb74LCxLQKTB5W","authorship_tag":"ABX9TyPHb8sJMiJE4o2Ocd2rsj0A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"snXgwZV2D26M"},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import LeaveOneOut\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n","from tensorflow.keras.applications.resnet50 import ResNet50"]},{"cell_type":"code","source":["\n","# carregar amostras de áudio e rotular com a classe correspondente\n","path = \"/content/drive/MyDrive/Parkinson_LONDON/frases\"\n","samples_with_parkinson = []\n","samples_without_parkinson = []\n","for filename in os.listdir(path):\n","    if \"com_parkinson\" in filename:\n","        sample, sr = librosa.load(os.path.join(path, filename))\n","        samples_with_parkinson.append((sample, 1))\n","    elif \"sem_parkinson\" in filename:\n","        sample, sr = librosa.load(os.path.join(path, filename))\n","        samples_without_parkinson.append((sample, 0))\n","\n","# embaralhar as listas\n","np.random.shuffle(samples_with_parkinson)\n","np.random.shuffle(samples_without_parkinson)\n","\n","# criar modelo CNN-2D pré-treinado ResNet-50\n","base_model = ResNet50(weights='imagenet', include_top=False)\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","predictions = Dense(1, activation='sigmoid')(x)\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n","\n","def extract_features(samples):\n","  features = []\n","  for sample in samples:\n","    x = librosa.feature.melspectrogram(sample[0], sr=22050)\n","    x = librosa.power_to_db(x, ref=np.max)\n","    x = np.expand_dims(x, axis=-1)\n","    x = np.expand_dims(x, axis=0)\n","    features.append(model.predict(x)[0])\n","  return features\n","\n","loo = LeaveOneOut()\n","X = np.array(samples_with_parkinson + samples_without_parkinson)\n","y = X[:, 1]\n","X = X[:, 0]\n","X_features = extract_features(X)\n","scores = []\n","for train_index, test_index in loo.split(X_features):\n","  # dividir dados em treinamento e teste\n","  X_train, X_test = X_features[train_index], X_features[test_index]\n","  y_train, y_test = y[train_index], y[test_index]\n","  # aumentar dados de treinamento\n","  datagen.fit(X_train)\n","    # treinar modelo\n","  model.fit(datagen.flow(X_train, y_train, batch_size=32),\n","  epochs=10, steps_per_epoch=len(X_train) // 32)\n","  # testar modelo\n","  score = model.evaluate(X_test, y_test)\n","  scores.append(score[1])\n","\n","accuracy = np.mean(scores)\n","print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"XkzsjzTQFMB7","executionInfo":{"status":"error","timestamp":1680544898340,"user_tz":180,"elapsed":2652,"user":{"displayName":"IGOR DE SOUZA LIMA","userId":"14410006608557562481"}},"outputId":"757c2eb9-5e84-4382-eb5f-535d32674c0b"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-32-2805d74ea8e1>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mloo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeaveOneOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples_with_parkinson\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msamples_without_parkinson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mX_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"]}]},{"cell_type":"code","source":["def extract_features(sample):\n","  x = librosa.feature.melspectrogram(sample, sr=22050)\n","  x = librosa.power_to_db(x, ref=np.max)\n","  x = np.expand_dims(x, axis=-1)\n","  return x\n","\n","\n","# carregar amostras de áudio e rotular com a classe correspondente\n","path = \"/content/drive/MyDrive/Parkinson_LONDON/frases\"\n","samples_with_parkinson = []\n","samples_without_parkinson = []\n","for filename in os.listdir(path):\n","    if \"1\" in filename:\n","        sample, sr = librosa.load(os.path.join(path, filename))\n","        samples_with_parkinson.append((sample, 1))\n","    elif \"0\" in filename:\n","        sample, sr = librosa.load(os.path.join(path, filename))\n","        samples_without_parkinson.append((sample, 0))\n","\n","# embaralhar as listas\n","np.random.shuffle(samples_with_parkinson)\n","np.random.shuffle(samples_without_parkinson)\n","\n","# criar modelo CNN-2D pré-treinado ResNet-50\n","base_model = ResNet50(weights='imagenet', include_top=False)\n","x = base_model.output\n","x = GlobalAveragePooling2D()(x)\n","predictions = Dense(1, activation='sigmoid')(x)\n","model = Model(inputs=base_model.input, outputs=predictions)\n","\n","loo = LeaveOneOut()\n","X = np.array(samples_with_parkinson + samples_without_parkinson)\n","y = X[:, 1]\n","X = X[:, 0]\n","X = [(sample, label) for sample, label in zip(X, y)]\n","X_features = extract_features([sample[0] for sample in X])\n","\n","for train_index, test_index in loo.split(X_features):\n","    X_train = [X[i][0] for i in train_index]\n","    y_train = [X[i][1] for i in train_index]\n","    X_test = [X[i][0] for i in test_index]\n","    y_test = [X[i][1] for i in test_index]\n","    X_train_features = extract_features(X_train)\n","    X_test_features = extract_features(X_test)\n","    X_train_augmented, y_train_augmented = augment_data(X_train_features, y_train)\n","    model.fit(X_train_augmented, y_train_augmented, validation_data=(X_test_features, y_test))\n","\n","\n","\n","# calcular a média das pontuações de precisão\n","accuracy = np.mean(scores)\n","print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":411},"id":"bO-LJuBrJALp","executionInfo":{"status":"error","timestamp":1680545693536,"user_tz":180,"elapsed":8148,"user":{"displayName":"IGOR DE SOUZA LIMA","userId":"14410006608557562481"}},"outputId":"cfa81bd1-00c3-4cff-890e-a73557992119"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-38-49df86983506>:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  X = np.array(samples_with_parkinson + samples_without_parkinson)\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-49df86983506>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mX_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-38-49df86983506>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(sample)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m22050\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpower_to_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: melspectrogram() takes 0 positional arguments but 1 positional argument (and 1 keyword-only argument) were given"]}]},{"cell_type":"code","source":["# Importar os pacotes necessários\n","import os\n","import numpy as np\n","import librosa\n","import librosa.display\n","from sklearn.model_selection import LeaveOneOut\n","from keras.models import Sequential\n","from keras.layers import GlobalAveragePooling2D, Dense\n","from keras.applications import ResNet50\n","\n","# Definir o caminho para os arquivos de áudio e os rótulos\n","data_dir = '/content/drive/MyDrive/Parkinson_LONDON/frases'\n","audio_files = os.listdir(data_dir)\n","labels = []\n","for file in audio_files:\n","    if '0' in file:\n","        labels.append(0)\n","    elif '1' in file:\n","        labels.append(1)\n","\n","# Definir as configurações da ResNet-50\n","input_shape = (224, 224, 3)\n","num_classes = 2\n","\n","# Definir a arquitetura da rede\n","conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n","model = Sequential()\n","model.add(conv_base)\n","model.add(GlobalAveragePooling2D())\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Definir as configurações de treinamento\n","learning_rate = 0.001\n","batch_size = 32\n","epochs = 10\n","\n","# Definir a técnica de validação cruzada Leave-One-Out\n","loo = LeaveOneOut()\n","\n","# Loop para treinar e avaliar o modelo usando Leave-One-Out\n","accuracy = []\n","for train_index, test_index in loo.split(audio_files):\n","    train_files, test_file = [audio_files[i] for i in train_index], audio_files[test_index[0]]\n","    train_data, train_labels = [], []\n","    for i, file in enumerate(train_files):\n","        y, sr = librosa.load(os.path.join(data_dir, file))\n","        #y = librosa.util.fix_length(y, 120000, mode='constant')\n","        spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n","        spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n","        spectrogram = np.expand_dims(spectrogram, axis=-1)\n","        spectrogram = np.repeat(spectrogram, 3, axis=-1)\n","        #spectrogram = librosa.util.pad_center(spectrogram, (input_shape[0], input_shape[1]))\n","        train_data.append(spectrogram)\n","        if '0' in file:\n","            train_labels.append(0)\n","        elif '1' in file:\n","            train_labels.append(1)\n","    test_data, test_labels = [], []\n","    y, sr = librosa.load(os.path.join(data_dir, test_file))\n","    #y = librosa.util.fix_length(y, 80000, mode='constant')\n","    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n","    spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n","    spectrogram = np.expand_dims(spectrogram, axis=-1)\n","    spectrogram = np.repeat(spectrogram, 3, axis=-1)\n","   # spectrogram = librosa.util.pad_center(spectrogram, input_shape[:-1])\n","    test_data.append(spectrogram)\n","    if '0' in test_file:\n","      test_labels.append(0)\n","    elif '1' in test_file:\n","      test_labels.append(1)\n","    # Compilar o modelo\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","    # Treinar o modelo\n","    model.fit(x=np.array(train_data), y=np.array(train_labels), epochs=epochs, batch_size=batch_size)\n","\n","    # Avaliar o modelo\n","    test_loss, test_acc = model.evaluate(x=np.array(test_data), y=np.array(test_labels))\n","    accuracy.append(test_acc)\n","\n","mean_accuracy = np.mean(accuracy)\n","print('Mean accuracy:', mean_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":289},"id":"efHs_7VEWrqj","executionInfo":{"status":"error","timestamp":1680546458830,"user_tz":180,"elapsed":21689,"user":{"displayName":"IGOR DE SOUZA LIMA","userId":"14410006608557562481"}},"outputId":"ea8d5579-ee19-4726-abb0-86aba72aa564"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-43-6a971ab617f1>:75: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  model.fit(x=np.array(train_data), y=np.array(train_labels), epochs=epochs, batch_size=batch_size)\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-6a971ab617f1>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# Treinar o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m# Avaliar o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (128,6011,3) into shape (128,)"]}]},{"cell_type":"code","source":["import os\n","import numpy as np\n","import librosa\n","from sklearn.model_selection import LeaveOneOut\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Define o caminho para a pasta com as amostras de áudio\n","folder_path = '/content/drive/MyDrive/Parkinson_LONDON/frases'\n","labels = []\n","\n","# Carrega as amostras de áudio e gera os espectrogramas de frequência\n","samples = []\n","for filename in os.listdir(folder_path):\n","    if filename.endswith('.wav'):\n","        label = 1 if 'parkinson' in filename else 0\n","        labels.append(label)\n","        file_path = os.path.join(folder_path, filename)\n","        signal, sr = librosa.load(file_path, sr=22050)\n","        max_length = 6508\n","        samples = [sample[:max_length] for sample in samples]\n","        spectrogram = librosa.feature.melspectrogram(y=signal, sr=sr, n_fft=2048, hop_length=512, n_mels=128)\n","        spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n","        spectrogram = spectrogram.astype(np.float32)\n","        spectrogram = np.expand_dims(spectrogram, axis=-1)\n","        samples.append(spectrogram)\n","\n","# Ajusta o tamanho das amostras\n","samples = pad_sequences(samples, maxlen=max_length, dtype='float32', padding='post', truncating='post', value=0.0)\n","\n","# Converte as listas para arrays numpy\n","samples = np.array(samples)\n","labels = np.array(labels)\n","\n","# Cria o modelo CNN-1D\n","model = Sequential()\n","model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=samples.shape[1:]))\n","model.add(MaxPooling1D(pool_size=2))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Compila o modelo\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Aplica a técnica de cross validation LOO\n","loo = LeaveOneOut()\n","accuracies = []\n","for train_index, test_index in loo.split(samples):\n","    # Separa os dados de treino e teste\n","    X_train, X_test = samples[train_index], samples[test_index]\n","    y_train, y_test = labels[train_index], labels[test_index]\n","    # Treina o modelo\n","    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)\n","    # Avalia o modelo no dado de teste e armazena a acurácia\n","    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n","    accuracies.append(accuracy)\n","\n","# Calcula a acurácia média e o desvio padrão\n","mean_accuracy = np.mean(accuracies)\n","std_accuracy = np.std(accuracies)\n","\n","print(\"Acurácia média: {:.2f}%\".format(mean_accuracy*100))\n","print(\"Desvio padrão: {:.2f}%\".format(std_accuracy*100))\n"],"metadata":{"id":"YPVOeedcXD65","colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"status":"error","timestamp":1680554587328,"user_tz":180,"elapsed":49808,"user":{"displayName":"IGOR DE SOUZA LIMA","userId":"14410006608557562481"}},"outputId":"7cb1a085-f7b2-46f0-b4d8-1e0919a9ad27"},"execution_count":1,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-eb15cf3f5414>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Ajusta o tamanho das amostras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Converte as listas para arrays numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   1132\u001b[0m                 \u001b[0;34mf\"Shape of sample {trunc.shape[1:]} of sequence at \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                 \u001b[0;34mf\"position {idx} is different from expected shape \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Shape of sample (6011, 1) of sequence at position 1 is different from expected shape (6508, 1)"]}]}]}